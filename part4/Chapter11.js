/**
 * [ PART IV. Advanced Engineering ]
 * Chapter 11. Full-Stack Integration (AI ì„œë¹„ìŠ¤ ì„œë²„ êµ¬í˜„)
 * * ìƒì„¸ ì£¼ì„: Node.js í™˜ê²½ì—ì„œ AI ëª¨ë¸ì„ ì„œë¹„ìŠ¤í•˜ê¸° ìœ„í•œ ë°±ì—”ë“œ ë¡œì§ì„ êµ¬í˜„í•©ë‹ˆë‹¤.
 */

// 1. [ê°€ìƒ í”„ë ˆì„ì›Œí¬ ì„¤ê³„] Express.js ìŠ¤íƒ€ì¼ì˜ ì„œë²„ ì‹œë®¬ë ˆì´ì…˜
const aiServer = {
    // ì„œë²„ì— ë¡œë“œëœ ê°€ìƒì˜ í•™ìŠµëœ ëª¨ë¸ (Chapter 07, 08ì˜ ê²°ê³¼ë¬¼)
    loadedModel: {
        predict: (data) => {
            console.log("ğŸ¤– AI ëª¨ë¸: ë°ì´í„° ë¶„ì„ ì¤‘...");
            // ì‹¤ì œë¡œëŠ” ì—¬ê¸°ì„œ í…ì„œ ì—°ì‚°ì´ ì¼ì–´ë‚©ë‹ˆë‹¤.
            return data > 0.5 ? "ì¸ì‹ ì„±ê³µ: ê³ ì–‘ì´" : "ì¸ì‹ ì„±ê³µ: ê°•ì•„ì§€";
        }
    },

    /**
     * 2. API ì—”ë“œí¬ì¸íŠ¸: ì´ë¯¸ì§€ ë¶„ì„ ìš”ì²­ ì²˜ë¦¬
     * @param {Object} request - í´ë¼ì´ì–¸íŠ¸ë¡œë¶€í„° ë°›ì€ ë°ì´í„°
     */
    handleInferenceRequest: async function (request) {
        console.log("\n[Server]: ë¶„ì„ ìš”ì²­ ìˆ˜ì‹ ...");

        try {
            // [ì „ì²˜ë¦¬ ë‹¨ê³„]: í´ë¼ì´ì–¸íŠ¸ê°€ ë³´ë‚¸ ì›ë³¸ ë°ì´í„°ë¥¼ ëª¨ë¸ì´ ì´í•´í•  ìˆ«ìë¡œ ë³€í™˜
            // ìˆ˜ì§„ ë‹˜ì´ ë°°ìš´ ë°ì´í„° ì •ê·œí™” ê³¼ì •ì…ë‹ˆë‹¤.
            const rawData = request.payload;
            const normalizedData = rawData / 255;

            // ë¹„ë™ê¸° AI ì¶”ë¡  (Inference)
            await new Promise(res => setTimeout(res, 500));
            const result = this.loadedModel.predict(normalizedData);

            return {
                status: 200,
                prediction: result,
                confidence: 0.98,
                timestamp: new Date().toISOString()
            };
        } catch (error) {
            return { status: 500, message: "Internal AI Error" };
        }
    }
};

/**
 * 3. í´ë¼ì´ì–¸íŠ¸(ëª¨ë°”ì¼/ë¸Œë¼ìš°ì €) í†µì‹  ì‹œë®¬ë ˆì´ì…˜
 */
async function startFullStackDemo() {
    console.log("ğŸš€ AI í’€ìŠ¤íƒ ì„œë¹„ìŠ¤ í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘...");

    // í´ë¼ì´ì–¸íŠ¸ê°€ ì„œë²„ì— ë³´ë‚¼ ê°€ìƒì˜ ì´ë¯¸ì§€ í”½ì…€ ë°ì´í„°
    const clientRequest = {
        payload: 200, // ê°€ìƒì˜ ì´ë¯¸ì§€ ë°ì´í„°
        userId: "Sujin_Park_08"
    };

    // ì„œë²„ API í˜¸ì¶œ
    const response = await aiServer.handleInferenceRequest(clientRequest);

    console.log("----------------------------------------");
    console.log("ğŸ“± [Client]: ì„œë²„ë¡œë¶€í„° ê²°ê³¼ ìˆ˜ì‹  ì™„ë£Œ!");
    console.log(`ğŸ“¡ ê²°ê³¼: ${response.prediction} (ì‹ ë¢°ë„: ${response.confidence * 100}%)`);
    console.log("----------------------------------------");
}

/**
 * [ ğŸ› ï¸ ë””ë²„ê¹… ê°€ì´ë“œ: ì‹œê°„ ì´ˆê³¼ (Timeout Issue) ]
 * * 1. í˜„ìƒ: í´ë¼ì´ì–¸íŠ¸ê°€ ìš”ì²­ì„ ë³´ëƒˆëŠ”ë° ì‘ë‹µì´ ë„ˆë¬´ ëŠ¦ì–´ ì•±ì´ ë©ˆì¶¤.
 * * 2. ì›ì¸ ë¶„ì„: AI ì¶”ë¡ (Inference)ì€ ë¬´ê±°ìš´ ì—°ì‚°ì´ë¼ ë©”ì¸ ì„œë²„ ìŠ¤ë ˆë“œë¥¼ ì ìœ í•  ìˆ˜ ìˆìŒ.
 * * 3. ì¬ë°œ ë°©ì§€ íŒ: 
 * - ì„œë²„ì—ì„œ 'Worker Threads'ë¥¼ ì‚¬ìš©í•˜ì—¬ AI ì—°ì‚°ì„ ë³„ë„ë¡œ ì²˜ë¦¬í•˜ì„¸ìš”.
 * - ì‘ë‹µì´ ì˜¤ë˜ ê±¸ë¦´ ê²½ìš° 'ì§„í–‰ë¥ (Progress Bar)'ì„ ë³´ë‚´ê±°ë‚˜ ë¹„ë™ê¸° í(Queue)ë¥¼ ë„ì…í•˜ì„¸ìš”.
 */

startFullStackDemo();

/*
[ ì‹¤í–‰ ë¦¬í¬íŠ¸ ]
- ë™ì‘ ì›ë¦¬: í´ë¼ì´ì–¸íŠ¸(Data) -> ì„œë²„(Pre-process) -> ëª¨ë¸(Predict) -> í´ë¼ì´ì–¸íŠ¸(Result)
- ì‹œê°„ ë³µì¡ë„: O(Inference_Time + Network_Latency)
*/