{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa10bf2a",
   "metadata": {},
   "source": [
    "# Chapter 03. ë¯¸ë¶„ê³¼ ìµœì í™” (Optimization)\n",
    "AIê°€ ìŠ¤ìŠ¤ë¡œ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ê³  ì˜¤ì°¨ë¥¼ ì¤„ì—¬ë‚˜ê°€ëŠ” ìˆ˜ì¹˜ ìµœì í™”ì˜ ì›ë¦¬ë¥¼ ì´í•´í•˜ê³ , ì‹¤ë¬´ì—ì„œ ì‚¬ìš©í•˜ëŠ” ê³ ë“± ì˜µí‹°ë§ˆì´ì €ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ë¶„ì„í•©ë‹ˆë‹¤.\n",
    "\n",
    "### 1. ì—°ì‡„ ë²•ì¹™(Chain Rule)ê³¼ ìë™ ë¯¸ë¶„ (Step 1)\n",
    "- **ëª©í‘œ**: í•©ì„±í•¨ìˆ˜ì˜ ë¯¸ë¶„ ì›ë¦¬ë¥¼ ì´í•´í•˜ê³  ì—­ì „íŒŒ(Backpropagation)ì˜ ê¸°ì´ˆ í™•ë¦½.\n",
    "- **Deep Dive**: ê³„ì‚° ê·¸ë˜í”„(Computational Graph)ë¥¼ í†µí•œ ì˜¤ì°¨ì˜ ì—­ë°©í–¥ ì „íŒŒ ìˆ˜ì‹ ì¦ëª….\n",
    "- **Engineering**: `Autograd` ì—”ì§„ì˜ ê¸°ë³¸ ì„¤ê³„ ì›ì¹™ ë° ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë¯¸ë¶„ê°’ ê´€ë¦¬ ê¸°ë²•.\n",
    "\n",
    "### 2. ê²½ì‚¬í•˜ê°•ë²•(Gradient Descent) ì§ì ‘ êµ¬í˜„ (Step 2)\n",
    "- **ëª©í‘œ**: ì†ì‹¤ í•¨ìˆ˜(Loss Function)ì˜ ìµœì†Ÿê°’ì„ ì°¾ì•„ê°€ëŠ” íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ë£¨í”„ ì„¤ê³„.\n",
    "- **Deep Dive**: MSE(Mean Squared Error)ì˜ í¸ë¯¸ë¶„ ìœ ë„ ë° í•™ìŠµë¥ (Learning Rate)ì— ë”°ë¥¸ ìˆ˜ë ´ ì†ë„ ë¶„ì„.\n",
    "- **Engineering**: NumPy ë²¡í„°í™” ì—°ì‚°ì„ í™œìš©í•˜ì—¬ ìˆ˜ì²œ ê°œì˜ íŒŒë¼ë¯¸í„°ë¥¼ í•œ ë²ˆì— ì—…ë°ì´íŠ¸í•˜ëŠ” ì„±ëŠ¥ ìµœì í™”.\n",
    "\n",
    "### 3. ê³ ë“± ì˜µí‹°ë§ˆì´ì €: Adam & RMSProp (Step 3)\n",
    "- **ëª©í‘œ**: ë°ì´í„°ì˜ íŠ¹ì„±ì— ë”°ë¼ í•™ìŠµë¥ ì„ ìŠ¤ìŠ¤ë¡œ ì¡°ì ˆí•˜ëŠ” Adaptive Optimizer ì´í•´.\n",
    "- **Deep Dive**: ì§€ìˆ˜ ì´ë™ í‰ê· (EMA)ì„ ì´ìš©í•œ ëª¨ë©˜í…€(Momentum)ê³¼ í•™ìŠµë¥  ê°ì‡  ìˆ˜ì‹ ë¶„ì„.\n",
    "- **Engineering**: í•™ìŠµ ì´ˆê¸° ë°œì‚°ì„ ë§‰ê¸° ìœ„í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë° ì˜µí‹°ë§ˆì´ì € ì„ íƒ ê°€ì´ë“œ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fbe8e8",
   "metadata": {},
   "source": [
    "# Chapter 03-1. ì—°ì‡„ ë²•ì¹™(Chain Rule)ê³¼ ìë™ ë¯¸ë¶„\n",
    "\n",
    "ì‹ ê²½ë§ì€ ìˆ˜ë§ì€ ì¸µ(Layer)ì´ ê²¹ì³ì§„ ê±°ëŒ€í•œ í•©ì„±í•¨ìˆ˜ì…ë‹ˆë‹¤. ê° ì¸µì˜ ê°€ì¤‘ì¹˜ê°€ ìµœì¢… ì˜¤ì°¨(Loss)ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ê³„ì‚°í•˜ê¸° ìœ„í•´ **ì—°ì‡„ ë²•ì¹™**ì„ ì‚¬ìš©í•˜ì—¬ ë¯¸ë¶„ê°’ì„ ë’¤ì—ì„œë¶€í„° ì•ìœ¼ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "\n",
    "> **Key Point**: $\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}$ ì›ë¦¬ë¥¼ ì´ìš©í•˜ë©´ ì•„ë¬´ë¦¬ ë³µì¡í•œ ì‹ ê²½ë§ë„ ë¶€ë¶„ì ì¸ ë¯¸ë¶„ì˜ ê³±ìœ¼ë¡œ í‘œí˜„ ê°€ëŠ¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ad6e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0: Loss = 49.8165, w = 1.49, b = 1.36\n",
      "Epoch 100: Loss = 0.8069, w = 2.80, b = 4.18\n",
      "Epoch 200: Loss = 0.8066, w = 2.77, b = 4.21\n",
      "Epoch 300: Loss = 0.8066, w = 2.77, b = 4.22\n",
      "Epoch 400: Loss = 0.8066, w = 2.77, b = 4.22\n",
      "--------------------------------------------------\n",
      "ìµœì¢… ê²°ê³¼ - ì˜ˆì¸¡ëœ ìˆ˜ì‹: y = 2.77X + 4.22\n",
      "ì‹¤ì œ ì •ë‹µ - ëª©í‘œ ìˆ˜ì‹: y = 3.00X + 4.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n[ ğŸ› ï¸ ì—”ì§€ë‹ˆì–´ë§ ë¶„ì„ ë¦¬í¬íŠ¸ ]\\n\\n1. ì‹œê°„ ë³µì¡ë„: O(Epochs * N)\\n   - ë°ì´í„° ê°œìˆ˜(N)ë§Œí¼ í–‰ë ¬ ì—°ì‚°ì„ ìˆ˜í–‰í•˜ê³  ì´ë¥¼ Epoch íšŸìˆ˜ë§Œí¼ ë°˜ë³µí•˜ë¯€ë¡œ ì„ í˜•ì ìœ¼ë¡œ ì¦ê°€í•©ë‹ˆë‹¤.\\n\\n2. ì‹¤í–‰ í™˜ê²½ ë° ì„±ëŠ¥ ìµœì í™”:\\n   - ë³¸ ì½”ë“œëŠ” NumPyì˜ ë²¡í„°í™” ì—°ì‚°(dot product)ì„ ì‚¬ìš©í•˜ì—¬ Pythonì˜ ì¼ë°˜ for ë£¨í”„ë³´ë‹¤ ìˆ˜ì²œ ë°° ë¹ ë¦…ë‹ˆë‹¤.\\n   - ë°ì´í„°ê°€ ì²œë§Œ ê±´ ì´ìƒìœ¼ë¡œ ëŠ˜ì–´ë‚  ê²½ìš°, ì „ì²´ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ê³„ì‚°í•˜ëŠ” 'Batch' ë°©ì‹ ëŒ€ì‹  'Mini-batch' ë°©ì‹ìœ¼ë¡œì˜ ì „í™˜ì´ í•„ìš”í•©ë‹ˆë‹¤.\\n\\n3. ì›ì¸ ë¶„ì„ (Engineering Insight):\\n   - ë§Œì•½ Lossê°€ ì¤„ì–´ë“¤ì§€ ì•Šê³  ë°œì‚°í•œë‹¤ë©´, learning_rateê°€ ë„ˆë¬´ ì»¤ì„œ ìµœì ì˜ ì§€ì ì„ ì§€ë‚˜ì³ë²„ë¦¬ëŠ” í˜„ìƒì…ë‹ˆë‹¤.\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. [ë°ì´í„° ì¤€ë¹„] ëª¨ë¸ì´ í•™ìŠµí•  ê°€ìƒì˜ ì •ë‹µ ë°ì´í„°ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "# np.random.seed(42): ë¬´ì‘ìœ„ ê²°ê³¼ë¥¼ ê³ ì •í•˜ì—¬ ì‹¤í–‰í•  ë•Œë§ˆë‹¤ ê°™ì€ ë°ì´í„°ê°€ ë‚˜ì˜¤ê²Œ í•©ë‹ˆë‹¤.\n",
    "np.random.seed(42)\n",
    "\n",
    "# X: ê³µë¶€ ì‹œê°„ (0~2ì‹œê°„ ì‚¬ì´ì˜ ëœë¤ê°’ 100ê°œ), y: ì‹¤ì œ ì„±ì  (y = 3X + 4 + ë…¸ì´ì¦ˆ)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "# 2. [íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”] ìš°ë¦¬ê°€ ì°¾ì•„ì•¼ í•  ì •ë‹µ(w=3, b=4)ì„ ì²˜ìŒì—” ëª¨ë¥´ëŠ” ìƒíƒœë¡œ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "# w: ê°€ì¤‘ì¹˜(Slope), b: í¸í–¥(Intercept). ì²˜ìŒì—” ë¬´ì‘ìœ„ ì (random)ì—ì„œ ì‹œì‘í•©ë‹ˆë‹¤.\n",
    "w = np.random.randn(1, 1)\n",
    "b = np.zeros((1, 1))\n",
    "\n",
    "# 3. [í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •] í•™ìŠµì˜ ì†ë„ì™€ íšŸìˆ˜ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.\n",
    "learning_rate = 0.1  # í•œ ë²ˆì˜ í•™ìŠµì—ì„œ ì–¼ë§ˆë‚˜ í° ë³´í­ìœ¼ë¡œ ì´ë™í• ì§€ ê²°ì •í•˜ëŠ” ìˆ˜ì¹˜\n",
    "epochs = 500         # ì „ì²´ ë°ì´í„°ë¥¼ ëª‡ ë²ˆ ë°˜ë³µí•´ì„œ í•™ìŠµí• ì§€ ê²°ì •\n",
    "\n",
    "# 4. [í•™ìŠµ ë£¨í”„] ë°˜ë³µë¬¸ì„ ëŒë©° ì ì§„ì ìœ¼ë¡œ ì˜¤ì°¨ë¥¼ ì¤„ì—¬ë‚˜ê°‘ë‹ˆë‹¤.\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # --- Step A. ì˜ˆì¸¡ (Forward Pass) ---\n",
    "    # í˜„ì¬ì˜ wì™€ bë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡ê°’(y_pred)ì„ ê³„ì‚°í•©ë‹ˆë‹¤. (ì¼ì°¨ë°©ì •ì‹: y = wX + b)\n",
    "    y_pred = X.dot(w) + b\n",
    "    \n",
    "    # --- Step B. ì˜¤ì°¨ ê³„ì‚° (Loss Calculation) ---\n",
    "    # ì‹¤ì œê°’(y)ê³¼ ì˜ˆì¸¡ê°’(y_pred)ì˜ ì°¨ì´ë¥¼ ì œê³±í•˜ì—¬ í‰ê· ì„ ë‚¸ MSE(Mean Squared Error)ë¥¼ êµ¬í•©ë‹ˆë‹¤.\n",
    "    # ì´ Loss ê°’ì´ 0ì— ê°€ê¹Œì›Œì§€ëŠ” ê²ƒì´ í•™ìŠµì˜ ìµœì¢… ëª©í‘œì…ë‹ˆë‹¤.\n",
    "    loss = np.mean((y_pred - y)**2)\n",
    "    \n",
    "    # --- Step C. ë¯¸ë¶„ (Backward Pass - Gradient Calculation) ---\n",
    "    # ìˆ˜ì‹: Lossë¥¼ wì™€ bë¡œ ê°ê° í¸ë¯¸ë¶„í•˜ì—¬ ì˜¤ì°¨ê°€ ì»¤ì§€ëŠ” 'ë°©í–¥'ì„ ì°¾ìŠµë‹ˆë‹¤.\n",
    "    # dw: ê°€ì¤‘ì¹˜ wë¥¼ ì–¼ë§ˆë‚˜ ìˆ˜ì •í•´ì•¼ í•˜ëŠ”ì§€ì— ëŒ€í•œ ë³€í™”ëŸ‰\n",
    "    # db: í¸í•¨ bë¥¼ ì–¼ë§ˆë‚˜ ìˆ˜ì •í•´ì•¼ í•˜ëŠ”ì§€ì— ëŒ€í•œ ë³€í™”ëŸ‰\n",
    "    # (2/100)ì€ ë¯¸ë¶„ ê³µì‹ì—ì„œ ë‚´ë ¤ì˜¤ëŠ” ì§€ìˆ˜ì™€ ë°ì´í„° ê°œìˆ˜(N=100)ë¥¼ ë‚˜ëˆˆ ê°’ì…ë‹ˆë‹¤.\n",
    "    dw = (2/100) * X.T.dot(y_pred - y)  # X.TëŠ” í–‰ë ¬ ê³±ì„ ìœ„í•´ Xë¥¼ ì „ì¹˜(Transpose)í•œ ê²ƒ dotì€ í–‰ë ¬ê³±ì…ˆ ë©”ì„œë“œ\n",
    "    db = (2/100) * np.sum(y_pred - y)    # í¸í–¥ì€ ë‹¨ìˆœíˆ ì˜¤ì°¨ë“¤ì˜ í•©ì— ë¹„ë¡€í•˜ì—¬ ì¡°ì •ë¨\n",
    "    \n",
    "    # --- Step D. ìµœì í™” (Parameter Update) ---\n",
    "    # ì°¾ì€ ë°©í–¥(dw, db)ì˜ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ learning_rateë§Œí¼ ì´ë™í•˜ì—¬ wì™€ bë¥¼ ê°±ì‹ í•©ë‹ˆë‹¤.\n",
    "    # ë§ˆì´ë„ˆìŠ¤(-)ë¥¼ ì“°ëŠ” ì´ìœ ëŠ” ê²½ì‚¬(Gradient)ê°€ ë‚®ì€ ê³³ìœ¼ë¡œ ë‚´ë ¤ê°€ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.\n",
    "    w = w - learning_rate * dw\n",
    "    b = b - learning_rate * db\n",
    "    \n",
    "    # 100ë²ˆ í•™ìŠµí•  ë•Œë§ˆë‹¤ í˜„ì¬ ìƒíƒœë¥¼ ì¶œë ¥í•˜ì—¬ ì˜ ë˜ê³  ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch:3d}: Loss = {loss:.4f}, w = {w[0][0]:.2f}, b = {b[0][0]:.2f}\")\n",
    "\n",
    "# 5. [ê²°ê³¼ í™•ì¸] í•™ìŠµì´ ëë‚œ í›„ ìµœì¢…ì ìœ¼ë¡œ ì°¾ì€ ìˆ˜ì‹ì„ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "print(\"-\" * 50)\n",
    "print(f\"ìµœì¢… ê²°ê³¼ - ì˜ˆì¸¡ëœ ìˆ˜ì‹: y = {w[0][0]:.2f}X + {b[0][0]:.2f}\")\n",
    "print(f\"ì‹¤ì œ ì •ë‹µ - ëª©í‘œ ìˆ˜ì‹: y = 3.00X + 4.00\")\n",
    "\n",
    "\"\"\"\n",
    "[ ğŸ› ï¸ ì—”ì§€ë‹ˆì–´ë§ ë¶„ì„ ë¦¬í¬íŠ¸ ]\n",
    "\n",
    "1. ì‹œê°„ ë³µì¡ë„: O(Epochs * N)\n",
    "   - ë°ì´í„° ê°œìˆ˜(N)ë§Œí¼ í–‰ë ¬ ì—°ì‚°ì„ ìˆ˜í–‰í•˜ê³  ì´ë¥¼ Epoch íšŸìˆ˜ë§Œí¼ ë°˜ë³µí•˜ë¯€ë¡œ ì„ í˜•ì ìœ¼ë¡œ ì¦ê°€í•©ë‹ˆë‹¤.\n",
    "\n",
    "2. ì‹¤í–‰ í™˜ê²½ ë° ì„±ëŠ¥ ìµœì í™”:\n",
    "   - ë³¸ ì½”ë“œëŠ” NumPyì˜ ë²¡í„°í™” ì—°ì‚°(dot product)ì„ ì‚¬ìš©í•˜ì—¬ Pythonì˜ ì¼ë°˜ for ë£¨í”„ë³´ë‹¤ ìˆ˜ì²œ ë°° ë¹ ë¦…ë‹ˆë‹¤.\n",
    "   - ë°ì´í„°ê°€ ì²œë§Œ ê±´ ì´ìƒìœ¼ë¡œ ëŠ˜ì–´ë‚  ê²½ìš°, ì „ì²´ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ê³„ì‚°í•˜ëŠ” 'Batch' ë°©ì‹ ëŒ€ì‹  'Mini-batch' ë°©ì‹ìœ¼ë¡œì˜ ì „í™˜ì´ í•„ìš”í•©ë‹ˆë‹¤.\n",
    "\n",
    "3. ì›ì¸ ë¶„ì„ (Engineering Insight):\n",
    "   - ë§Œì•½ Lossê°€ ì¤„ì–´ë“¤ì§€ ì•Šê³  ë°œì‚°í•œë‹¤ë©´, learning_rateê°€ ë„ˆë¬´ ì»¤ì„œ ìµœì ì˜ ì§€ì ì„ ì§€ë‚˜ì³ë²„ë¦¬ëŠ” í˜„ìƒì…ë‹ˆë‹¤.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd415272",
   "metadata": {},
   "source": [
    "### ğŸ’¡ ë””ë²„ê¹… ê°€ì´ë“œ: í•™ìŠµë¥ (Learning Rate) ì„¤ì • ì—ëŸ¬\n",
    "\n",
    "> **[ì›ì¸ ë¶„ì„]**\n",
    "> 1. **Learning Rateê°€ ë„ˆë¬´ í´ ë•Œ**: ìµœì†Ÿê°’ì„ ì§€ë‚˜ì³ ë°œì‚°(Exploding)í•˜ë©° Lossê°€ `inf` ë˜ëŠ” `NaN`ì´ ë¨.\n",
    "> 2. **Learning Rateê°€ ë„ˆë¬´ ì‘ì„ ë•Œ**: ìµœì†Ÿê°’ì— ë„ë‹¬í•˜ê¸° ì „ì— í•™ìŠµì´ ëë‚˜ê±°ë‚˜, Local Minimumì— ê°‡í˜€ ì„±ëŠ¥ì´ ì •ì²´ë¨.\n",
    "\n",
    "> **[ì¬ë°œ ë°©ì§€ íŒ]**\n",
    "> 1. í•™ìŠµ ì´ˆê¸°ì—ëŠ” `0.1`, `0.01`, `0.001` ë“± ë¡œê·¸ ìŠ¤ì¼€ì¼ë¡œ ì‹œë„í•˜ë©° ì ì • ë²”ìœ„ë¥¼ ì°¾ìœ¼ì„¸ìš”.\n",
    "> 2. `np.isnan(loss)` ì¡°ê±´ì„ ì¶”ê°€í•˜ì—¬ ë°œì‚° ì‹œ í•™ìŠµì„ ì¡°ê¸° ì¢…ë£Œ(Early Stopping)í•˜ëŠ” ë¡œì§ì„ ê²€í† í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e215f1",
   "metadata": {},
   "source": [
    "# Chapter 03-3. ê³ ë“± ì˜µí‹°ë§ˆì´ì €: Adam & RMSProp\n",
    "\n",
    "ë‹¨ìˆœ ê²½ì‚¬í•˜ê°•ë²•(SGD)ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ í˜„ëŒ€ ë”¥ëŸ¬ë‹ì—ì„œëŠ” **Adam**ì„ ê°€ì¥ í‘œì¤€ì ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "1. **Momentum (ê´€ì„±)**: ê°€ë˜ ë°©í–¥ìœ¼ë¡œ ê³„ì† ê°€ë ¤ëŠ” ì„±ì§ˆì„ ì´ìš©í•´ Local Minimumì„ íƒˆì¶œí•©ë‹ˆë‹¤.\n",
    "2. **RMSProp (ì ì‘ì  í•™ìŠµë¥ )**: ë³€í™”ê°€ ì ì€ íŒŒë¼ë¯¸í„°ëŠ” í¬ê²Œ, ë³€í™”ê°€ í° íŒŒë¼ë¯¸í„°ëŠ” ì‘ê²Œ í•™ìŠµë¥ ì„ ì¡°ì ˆí•˜ì—¬ ì•ˆì •ì ìœ¼ë¡œ ìˆ˜ë ´ì‹œí‚µë‹ˆë‹¤.\n",
    "3. **Adam**: ìœ„ ë‘ ê°€ì§€ ì¥ì ì„ ê²°í•©í•œ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ, ëŒ€ë¶€ë¶„ì˜ ë”¥ëŸ¬ë‹ ë¬¸ì œì—ì„œ ê¸°ë³¸ ì˜µì…˜ìœ¼ë¡œ ì„ íƒë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06aff929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: Loss = 26.6722\n",
      "Epoch 200: Loss = 13.0609\n",
      "Epoch 300: Loss = 5.9769\n",
      "\n",
      "Adam ìµœì¢… ê²°ê³¼ - w: 2.34, b: 2.36\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n[ ğŸ› ï¸ ìƒì„¸ ì£¼ì„ ë° ê¸°ëŠ¥ ì„¤ëª… ]\\n- m_w (Momentum): ê¸°ìš¸ê¸°ì˜ ë°©í–¥ì„±ì„ ìœ ì§€í•˜ì—¬ ì§„ë™ì„ ì¤„ì„.\\n- v_w (Velocity): ê¸°ìš¸ê¸°ì˜ ì œê³±ê°’ì„ ëˆ„ì í•˜ì—¬ íŒŒë¼ë¯¸í„°ë³„ë¡œ í•™ìŠµ ë³´í­ì„ ì¡°ì ˆ.\\n- epsilon: ë¶„ëª¨ê°€ 0ì´ ë˜ì–´ ì—ëŸ¬ê°€ ë°œìƒí•˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ëŠ” ì•ˆì „ì¥ì¹˜.\\n\\n- ì‹œê°„ ë³µì¡ë„: O(Epochs * N) (ì¶”ê°€ ì—°ì‚°ëŸ‰ì´ ì ìœ¼ë©´ì„œ ìˆ˜ë ´ ì†ë„ëŠ” í›¨ì”¬ ë¹ ë¦„)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# [ë°ì´í„° ì¤€ë¹„] ì´ì „ ì‹¤ìŠµê³¼ ë™ì¼í•œ ì„ í˜• ë°ì´í„°\n",
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "# [íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”]\n",
    "w = np.random.randn(1, 1)\n",
    "b = np.zeros((1, 1))\n",
    "\n",
    "# [Adam í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •]\n",
    "learning_rate = 0.01\n",
    "beta1 = 0.9   # ëª¨ë©˜í…€(ê´€ì„±) ê³„ìˆ˜\n",
    "beta2 = 0.999 # ì ì‘ì  í•™ìŠµë¥ (ì§€ìˆ˜ ì´ë™ í‰ê· ) ê³„ìˆ˜\n",
    "epsilon = 1e-8 # 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•œ ì•„ì£¼ ì‘ì€ ê°’\n",
    "\n",
    "# ê´€ì„±ê³¼ ì†ë„ë¥¼ ì €ì¥í•  ë³€ìˆ˜ ì´ˆê¸°í™” (ëª¨ì–‘ì€ íŒŒë¼ë¯¸í„°ì™€ ë™ì¼í•˜ê²Œ)\n",
    "m_w, v_w = 0, 0\n",
    "m_b, v_b = 0, 0\n",
    "\n",
    "# [í•™ìŠµ ë£¨í”„]\n",
    "for epoch in range(1, 301):\n",
    "    # 1. Forward Pass (ì˜ˆì¸¡)\n",
    "    y_p = X.dot(w) + b\n",
    "    \n",
    "    # 2. Gradient ê³„ì‚° (ì´ì „ ë‹¨ê³„ì—ì„œ ë°°ìš´ X.T.dot í™œìš©)\n",
    "    dw = (2/100) * X.T.dot(y_p - y)\n",
    "    db = (2/100) * np.sum(y_p - y)\n",
    "    \n",
    "    # 3. Adam ì—…ë°ì´íŠ¸ ë¡œì§ (í•µì‹¬ ê¸°ëŠ¥)\n",
    "    # Step 1: ëª¨ë©˜í…€ ì—…ë°ì´íŠ¸ (ê°€ë˜ ë°©í–¥ ê¸°ì–µ)\n",
    "    m_w = beta1 * m_w + (1 - beta1) * dw\n",
    "    m_b = beta1 * m_b + (1 - beta1) * db\n",
    "    \n",
    "    # Step 2: ê°œë³„ í•™ìŠµë¥  ì¡°ì ˆ (ê¸°ìš¸ê¸° í¬ê¸° ê¸°ì–µ)\n",
    "    v_w = beta2 * v_w + (1 - beta2) * (dw**2)\n",
    "    v_b = beta2 * v_b + (1 - beta2) * (db**2)\n",
    "    \n",
    "    # Step 3: í¸í–¥ ë³´ì • (ì´ˆê¸° í•™ìŠµ ì•ˆì •í™”)\n",
    "    m_w_hat = m_w / (1 - beta1**epoch)\n",
    "    m_b_hat = m_b / (1 - beta1**epoch)\n",
    "    v_w_hat = v_w / (1 - beta2**epoch)\n",
    "    v_b_hat = v_b / (1 - beta2**epoch)\n",
    "    \n",
    "    # Step 4: íŒŒë¼ë¯¸í„° ìµœì¢… ì—…ë°ì´íŠ¸\n",
    "    # ë¶„ëª¨ì˜ sqrt(v_hat)ì´ í•™ìŠµë¥ ì„ ìë™ìœ¼ë¡œ ì¡°ì ˆí•´ì¤Œ\n",
    "    w = w - learning_rate * m_w_hat / (np.sqrt(v_w_hat) + epsilon)\n",
    "    b = b - learning_rate * m_b_hat / (np.sqrt(v_b_hat) + epsilon)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        loss = np.mean((X.dot(w) + b - y)**2)\n",
    "        print(f\"Epoch {epoch}: Loss = {loss:.4f}\")\n",
    "\n",
    "print(f\"\\nAdam ìµœì¢… ê²°ê³¼ - w: {w[0][0]:.2f}, b: {b[0][0]:.2f}\")\n",
    "\n",
    "\"\"\"\n",
    "[ ğŸ› ï¸ ìƒì„¸ ì£¼ì„ ë° ê¸°ëŠ¥ ì„¤ëª… ]\n",
    "- m_w (Momentum): ê¸°ìš¸ê¸°ì˜ ë°©í–¥ì„±ì„ ìœ ì§€í•˜ì—¬ ì§„ë™ì„ ì¤„ì„.\n",
    "- v_w (Velocity): ê¸°ìš¸ê¸°ì˜ ì œê³±ê°’ì„ ëˆ„ì í•˜ì—¬ íŒŒë¼ë¯¸í„°ë³„ë¡œ í•™ìŠµ ë³´í­ì„ ì¡°ì ˆ.\n",
    "- epsilon: ë¶„ëª¨ê°€ 0ì´ ë˜ì–´ ì—ëŸ¬ê°€ ë°œìƒí•˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ëŠ” ì•ˆì „ì¥ì¹˜.\n",
    "\n",
    "- ì‹œê°„ ë³µì¡ë„: O(Epochs * N) (ì¶”ê°€ ì—°ì‚°ëŸ‰ì´ ì ìœ¼ë©´ì„œ ìˆ˜ë ´ ì†ë„ëŠ” í›¨ì”¬ ë¹ ë¦„)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea45eca",
   "metadata": {},
   "source": [
    "### ğŸ’¡ ë””ë²„ê¹… ê°€ì´ë“œ: ì˜µí‹°ë§ˆì´ì € ì„ íƒ ì¥ì• \n",
    "\n",
    "> **[ì›ì¸ ë¶„ì„]**\n",
    "> ì´ˆë³´ ì—”ì§€ë‹ˆì–´ëŠ” ì–´ë–¤ ì˜µí‹°ë§ˆì´ì €ë¥¼ ì“¸ì§€ ê³ ë¯¼í•˜ë‹¤ ì‹œê°„ì„ ë‚­ë¹„í•˜ê³¤ í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ë°ì´í„°ì˜ íŠ¹ì„±(í¬ì†Œì„±, ë…¸ì´ì¦ˆ ë“±)ì— ë”°ë¼ ìµœì ì˜ ì•Œê³ ë¦¬ì¦˜ì€ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "> **[ì¬ë°œ ë°©ì§€ íŒ]**\n",
    "> 1. **ê³ ë¯¼ë  ë• Adam**: ëŒ€ë¶€ë¶„ì˜ ê²½ìš° Adamì´ ê°€ì¥ ë¹ ë¥´ê³  ì•ˆì •ì ì…ë‹ˆë‹¤.\n",
    "> 2. **ì •êµí•œ íŠœë‹ì€ SGD + Momentum**: ìµœì¢… ëª¨ë¸ì˜ ì •í™•ë„ë¥¼ ê·¹ëŒ€í™”í•  ë•ŒëŠ” Adamë³´ë‹¤ ë‹¨ìˆœí•œ ë°©ì‹ì´ ë” ì„¸ë°€í•˜ê²Œ ì¡°ì •ë  ë•Œê°€ ìˆìŠµë‹ˆë‹¤.\n",
    "> 3. **í•™ìŠµ ê³¡ì„  ëª¨ë‹ˆí„°ë§**: Lossê°€ ì¶¤ì„ ì¶”ë“¯ ìš”ë™ì¹œë‹¤ë©´ `learning_rate`ë¥¼ ë‚®ì¶”ê±°ë‚˜ `beta1` ê°’ì„ ì¡°ì •í•´ ë³´ì„¸ìš”."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
